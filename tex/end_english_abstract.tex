%# -*- coding: utf-8-unix -*-
\begin{bigabstract}
“Warehouse scale computers” (WSCs) house large-scale web applications and cloud services. The cost of construction and operation of these datacenters ranges from tens to hundreds of millions of dollars. As more computing moves into the cloud, it is becoming exceedingly important to leverage the resources in WSCs as efficiently as possible. However, the utilization of the computing resources in modern WSCs remains low, often not exceeding 20\%.

A primary reason for the low utilization is the popularity of latency-critical (LC) services such as social media, search engines, software-as-a-service, online maps, webmail, machine translation, online shopping and advertising. These user-facing services are typically scaled across thousands of servers and access distributed state stored in memory or Flash across these servers. While their load varies significantly due to diurnal patterns and unpredictable spikes in user accesses, it is difficult to consolidate load on a subset of highly utilized servers because the application state does not fit in a small number of servers and moving state is expensive. The cost of such underutilization can be significant. For instance, Google web\_search servers often have an average idleness of 30\% over a 24 hour period. For a hypothetical cluster of 10,000 servers, this idleness translates to a wasted capacity of 3,000 servers.

Each machine in the datacenter house numerous cores, often 4 to 8 cores per socket, and 2 to 4 sockets per machine.  A promising way to improve efficiency is to launch best-effort batch (BE) tasks on the same servers and exploit any resources underutilized by LC workloads. Batch analytics frameworks can generate numerous BE tasks and derive significant value even if these tasks are occasionally deferred or restarted. However, in light of the significant potential for parallelism on a single machine, there are a number of resources shared among cores. The main challenge of this approach is interference between colocated workloads on shared resources such as caches, memory, I/O channels, and network links. LC tasks operate with strict service level objectives (SLOs) on tail latency, and even small amounts of interference could result in performance interference across-cores, negatively and unpredictably impacting the quality of service (QoS) of user-facing and latency sensitive application threads. Hence, some of the past work on workload colocation focused only on throughput workloads. More recent systems predict or detect when an LC task suffers significant interference from the colocated tasks, and avoid or terminate the colocation. These systems protect LC workloads, but leaving cores idle, and resulting in an over-provisioning that reduce the opportunities for higher utilization of the entire datacenter through colocation, 

Recently introduced hardware features like cache isolation and fine-grained power control allow us to improve colocation. Our goal is to eliminate SLO violations at all levels of loads for the LC job while maximizing the throughput for BE tasks by coordinating various isolation mechanisms on shared resources.

The primary shared resource in the server are the cores in the one or more CPU sockets. We cannot simply statically partition cores between the LC and BE tasks using mechanisms such as \textit{cgroup}. When user-facing services such as web search face a load spike, they need all available cores to meet throughput demands without latency SLO violations. Similarly, we cannot simply assign high priority to LC tasks and rely on OS-level scheduling of cores between tasks. Common scheduling algorithms such as Linux’s completely fair scheduler have vulnerabilities that lead to frequent SLO violations when LC tasks are colocated with BE tasks. Real-time scheduling algorithms (e.g., SCHED\_FIFO) are not work-preserving and lead to lower utilization. 

There are many other shared resources, like the last-level cache(LLC), DRAM bandwidth and some less obvious ones like power. A major challenge with colocation is cross-resource interactions. A BE task can cause interference in all the shared resources discussed. Similarly, many LC tasks are sensitive to interference on multiple resources. Therefore, it is not sufficient to manage one source of interference: all potential sources need to be monitored and carefully isolated if need be. In addition, interference sources interact with each other. For example, LLC contention causes both types of tasks to require more DRAM bandwidth, also creating a DRAM bandwidth bottleneck. Similarly, a task that notices network congestion may attempt to use compression, causing core and power contention. In theory, the number of possible interactions scales with the square of the number of interference sources, making this a very difficult problem.

In summary, there are several challenges towards this goal. First, we must carefully share each individual resource; conservative allocation will minimize the throughput for BE tasks, while optimistic allocation will lead to SLO violations for the LC tasks. Second, the performance of both types of tasks depends on multiple resources, which leads to a large allocation space that must be explored in real-time as load changes. Finally, there are non-obvious interactions between isolated and non-isolated resources in modern servers. For instance, increasing the cache allocation for an LC task to avoid evictions of hot data may create memory bandwidth interference due to the increased misses for BE tasks.

Machine learning is the study of computer programs and algorithms that learn about their environment and improve automatically with experience. Within this larger framework, Reinforcement Learning (RL), sometimes called “learning from interaction,” studies how autonomous agents situated in stochastic environments can learn to maximize the cumulative sum of a numerical reward signal received over their lifetimes through interaction with their environment. RL has proven its extraordinary ability to solve extremely complex problems, and very often, even out-performing the best of humankind.

In this paper, we discussed the application of reinforcement learning in the domain of resource management, aiming to enable aggressive colocation of LC workloads and BE jobs with an RL agent automatically coordinating multiple isolation mechanisms in modern servers. There are three major challenges facing an RL agent. \textbf{Temporal credit assignment.} The agent needs to learn how to assign credit and blame to past actions for each observed immediate reward. In some cases, a seemingly desirable action that yields high immediate reward may drive the system towards undesirable, stagnant states that oﬀer no rewards; at other times, executing an action with no immediate reward may be critical to reaching desirable future states. Hence, acting optimally requires planning: the agent must anticipate the future consequences of its actions and act accordingly to maximize its long-term cumulative payoﬀs. \textbf{Exploration vs. exploitation.} The agent needs to explore its environment sufficiently (and collect training data) before it can learn a high-performance control policy, but it also needs to exploit the best policy it has found at any point in time. Too little exploration of the environment can cause the agent to commit to suboptimal policies early on, whereas excessive exploration can result in long periods during which the agent executes suboptimal actions to explore its environment. Furthermore, the agent needs to continue exploring its environment and improving its policy (life-long learning) to accommodate changes in its environment (e.g., due to phase changes or context switches). \textbf{Generalization.} Because the size of the state space is exponential in the number of attributes considered, the agent’s environment may be represented by an overwhelming number of possible states. In such cases, it is exceedingly improbable for the agent to experience the same state more than once over its lifetime. Consequently, the only way to learn a mapping from states to actions is to generalize and apply the experience gathered over previously encountered (but different) system states to act successfully in new states.

\end{bigabstract}