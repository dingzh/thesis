\chapter{结论}
本文探讨了通过强化学习方法协调多种资源隔离机制来解决数据中心任务混合执行的问题。通过设置对象缓存服务和期权定价任务混合执行的实验，我们证明，如果合理的设置奖赏信号，智能体的确能够从环境中学习到合理的资源分配策略，在保证延迟敏感型任务的服务质量和提高数据中心服务器资源利用率。相比于基于设置调度策略的算法，强化学习的资源调度方法只需要制定合理的奖赏函数，而不需要设置任务性能和资源调度的专业知识。然而，智能体学习到的的调度策略的质量还有待量化的分析。

实验中，我们也发现了强化学习在资源调度应用还存在诸多难题。首先是训练数据量的问题。与所有的机器学习任务一样，强化学习的结果依赖于学习的样本，而受延迟敏感型任务的服务质量定义方法的影响，强化学习的学习样本采样率较低。例如前文提到的Elgg社交服务引擎，其服务质量被定义为60秒内所有请求的延迟的九十分位值。在单线程的学习模型中，需要一分钟的时间，环境才能返回上一个动作产生的结果，这极大的影响了学习速度。通过减少采样的时间可以提高采样率，然而采样的时间越短，延迟敏感型任务完成的请求数也就越少，数据的方差也会越大，以至于不稳定或不能反映真实的延迟敏感型任务的服务质量。受限于实验环境，我们无法通过大规模的并行学习来提高学习速度；我们通过两个方法减轻此问题的影响。首先，在模型的设计上我们使用的记忆重放来提高数据的利用率。其次，我们选择的任务为对象缓存服务，相比社交引擎，其具有很高的吞吐量，每秒服务的请求在三十万左右。对象缓存服务高吞吐量的特点使得我们可以在较短时间内测试其服务质量。即便如此，实验的模拟环境中，启动测试程序模拟如此大的请求量也比较耗时，所以最终获得学习样本的速度也不高，但相比Elgg社交服务有很大提升。

通过服务器运行的离线数据进行学习可以部分解决训练数据量的问题，然而这将限制智能体探索不同动作的能力——智能体只能学习到服务器上使用过的资源分配方案。例如，若服务器上运行的资源分配策略限制至少为延迟敏感型任务分配一半的计算核心，那么智能体无法从中知道分配更少的核心将产生怎样的影响，显然生产环境的服务器不允许智能体做这样的探索。

另一个挑战则是奖赏函数的设计问题。实验中我们设置的奖赏信号基于测得的对象缓存服务的延迟所落在的区间。在这样的设置下，智能体可以区分使得延迟敏感型任务违反服务质量的资源分配和其他保证其服务质量的资源分配。然而，如果在不同的资源分配方案下，
延迟敏感型任务产生接近的延迟，智能体将不能区分他们表现的好坏。这在延迟敏感型任务对某些共享资源的分配并不敏感时影响智能体学得的分配策略的效率。例如延迟敏感型策略在某些情况下对缓存容量不敏感，那么智能体会认为对其分配多少缓存没有区别，可能的情况则是给延迟敏感型任务分配了更多不必要的缓存。在实验中，我们通过测试延迟敏感型任务的服务质量随资源分配的变化来确定其敏感性，并在动作空间中消除一些智能体无法区分的动作。例如，延迟敏感型任务在获得10核心，50\%缓存与10核心，60\%缓存时有几乎一致的服务性能，我们将不允许智能体做出后一资源分配决策。

强化学习的优化，是基于对累积奖赏期望的优化。这样的优化目标对于大部分任务都是合理的。然而对于延迟敏感型任务，其实际负载常出现无法预料的高峰。由于高峰出现的概率大大低于其他负载强度出现的概率，智能体的学习过程中极可能认为在负载高峰时违反服务质量要求而受到的惩罚的期望是可以接受的；这显然与我们实际的预期不符。我们的实验中简化了实际情况，忽略了负载高峰的存在。通过修改奖励信号，提高在这种情况下智能体受到的惩罚可以避免智能体忽略对负载高峰时资源分配的优化，然而更复杂化的奖励信号将如何影响整个强化学习的过程还难以得知。

综上所述，强化学习在数据中心的资源调度中有很大的潜力，同时也有很多问题需要解决。如果强化学习可以实际应用到时间数据中心资源调度决策中，将有希望解决其低资源利用率问题。