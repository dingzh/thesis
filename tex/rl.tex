\chapter{强化学习方法}
延迟敏感型任务和批处理任务的混合执行能显著提高服务器的资源利用率。气泡测试等方法也使得我们能够以可接受的代价找的合适的混合策略。借助上文提到的基于硬件或软件的资源隔离方法，延迟敏感型任务的服务质量可以得到有效保证。然而，静态的混合执行策略无法充分得利用服务器资源\---任何的静态混合执行策略要么过于保守，导致资源利用率不足；要么过于乐观，导致延迟敏感型任务违反其服务质量要求\cite{lo2015heracles}
\section{动态资源调度的动机}
当两个或多个任务并行执行时，任务之间竞争共享的资源。本节分析混合执行中存在的主要资源竞争和进行动态资源管理的动机。

\subsection{处理器核心}
单个服务器中最主要的共享资源是一个或多个处理器接口中的处理器核心。我们不能简单的将核心通过cgroup静态地分配给延迟敏感型任务和批处理任务。当延迟敏感型任务，例如搜索引擎，达到吞吐量高峰时，需要所有可用的核心来满足负载需求以避免违反服务质量要求。仅仅给延迟敏感型任务分配更高的进程调度优先级并不能解决问题。例如Linux系统的完全公平调度器（Completely Fair Scheduler, CFS)在内的常见调度器因为自身的缺陷，在延迟敏感型任务和批处理任务混合执行时常常导致延迟敏感型任务违反服务质量要求\cite{leverich2014reconciling}。实时调度算法，例如Linux的SCHED\_FIFO并不保存已完成的运算，导致系统资源实际利用率下降。英特尔处理器的超线程技术使得问题更加复杂。

\subsection{末级高速缓存}
多个研究表明共享末级高速缓存中不受控制的干扰会严重影响混合执行任务的性能\cite{delimitrou2014quasar}\cite{govindan2011cuanta}\cite{leverich2014reconciling}\cite{mars2012increasing}\cite{sanchez2011vantage}。当混合执行的任务中含有面向用户的延迟敏感型任务时，末级高速缓存的动态管理更加重要，因为轻微的相互干扰会引起巨大的延迟\cite{kasture2014ubik}。同时末级高速缓存的动态管理在含有面向用户的延迟敏感型任务也更加困难，因为这类任务的缓存占用随着其负载强度的变化而产生改变\cite{leverich2014reconciling}。

\subsection{网络带宽}
数据中心的大型应用程序主要是通过向外扩展（scaling-out）达到其规模，因此服务器之间的网络连接性能也至关重要。因为延迟敏感型任务产生的网络流量随负载强度变化，网络带宽的管理也应动态进行。静态的优先级设置会导致网络连接的低利用率和批处理任务的饥饿（starvation）\cite{pattara2002starvation}。对网络带宽的动态管理也可以延伸至对固态存储器（Solid State Storage)访问带宽的管理上\cite{seong2010hydra}。

\subsection{热功率}
因处理器总体热设计功耗（Themal Design Power）限制，同一处理器接口上的核心之间会相互影响其最大时钟频率，进而影响计算性能。为保证运行延迟敏感型任务的核心不会因为运行批处理任务核心过度能耗使用而导致时钟频率的突然下降，必须限制运行批处理任务核心的时钟频率。静态将运行批处理任务的核心设置在最低时钟频率可以避免运行延迟敏感型任务的核心因能耗而被限制性能。然而使用静态的设置严重影响了批处理任务运行的性能。大部分的批处理任务并不总是会使核心处于高能耗状态，并且延迟敏感型任务也仅仅在高负载强度时才需要提升核心频率来获得额外的性能提升。为了保证延迟敏感型任务在高负载时获得时钟频率的保证的同时，尽可能避免过分限制批处理任务的性能，动态的能耗控制是必要的。

\section{强化学习应用在动态资源调度}
混合执行延迟敏感型任务和批处理任务的更大挑战在于控制两种任务同时在上述多种资源上复杂的相互干扰。批处理任务会同时在各种共享资源上对延迟敏感型任务产生干扰，而延迟敏感型任务对来自各种共享资源上的干扰都十分敏感。因此，仅仅控制好一种共享资源上的相互干扰是不足以保证延迟敏感型任务的服务质量的；所有共享资源都应尽可能的监控并有效的实施干扰隔离。除此之外，各种共享资源上的干扰还会相互影响。例如，延迟敏感型任务和批处理任务在缓存上的竞争会导致某一任务对动态随机存储访问带宽的要求提高，成为潜在的性能瓶颈；或是任务发现网络连接的拥塞会采取更激进的压缩算法，导致计算资源和能耗的竞争。各种资源的相互竞争导致保证延迟敏感型任务的服务质量的同时，通过动态资源调度提升服务器资源利用率难以实现。

\begin{figure}
  \centering
  \begin{minipage}[b]{0.7\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{rl/rl.png}
    \captionof{figure}{(a) 智能体与环境交互过程\qqqa（b）作为资源调度器的强化学习智能体}
    \label{fig:rl}
  \end{minipage}     
\end{figure}


强化学习在一些极具挑战性的决策类优化问题中表现出其巨大潜力\cite{mnih2013playing}\cite{silver2016mastering}。强化学习从智能体（agents）过往的经验学习使得智能体能做出更好的决策。图\ref{fig:rl}（a）展示了智能体如何与环境交互。交互过程被离散化成为一连串步骤，每一步智能体先感知环境的状态（state）为$s_t$，然后选择并执行一个动作（action）。这一步骤将导致环境的状态发生变化，转移至$s_{t+1}$，智能体在后续步骤中也将能够感知，此状态的变化还将生成一个奖励信号（reward）$r_t$。智能体对任务本身和规则一无所知，仅仅通过环境返回的奖励信号来的得知他当前学习到策略的好坏\cite{bertsekas1995dynamic}，它学习的目的在优化自身选择动作的策略于以最大化其从长远来看获得的累积奖赏。强化学习已经有久远的历史，但最近研究将强化学习和深度学习相结合使其在很多应用上取得很好效果，比如玩电子游戏\cite{mnih2015human}，控制数据中心的冷却系统\cite{evans2016deepmind}。

回顾优化数据中心资源调度的问题，我们相信强化学习非常适合解决此类问题。首先，资源调度程序做出的决策通常有很高的重复性，因此可以产生大量数据用于强化学习训练。其次，正如我们在强化学习算法应用在游戏\cite{mnih2015human}\cite{evans2016deepmind}时看到的，强化学习算法可以通过深度神经网络（Deep Neural Network, DNN）模拟复杂的模型和决策过程。不同的信号和环境可能存在的轻微扰动都可以整合起来同时输入神经网络，产生一个决策模型。另外，通过强化学习，我们能够通过一个相关的奖励信号来优化一些难以直接优化的目标。最后，通过持续的学习，强化学习可以从大量经验中学到不同情况下的优化策略。

图\ref{fig:rl}（b）展示了如何在强化学习的框架内优化数据中心的资源调度以提高服务器利用率。智能体在这里充当资源调度器，而环境则包含了混合执行的延迟敏感型任务和批处理任务，执行这些任务的所有服务器资源，以及按一定模式变化的延迟敏感型任务的负载。强化学习的每一步骤对应资源调度器的一次决策，智能体将在此时获得环境的状态并做出动作。环境相关的信息包括当前延迟敏感型任务的负载强度，计算核心，末级缓存和随机存储访存带宽等等资源的利用率，以及延迟敏感型任务的服务质量。智能体可做的选择则为增加分配给延迟敏感型任务的某类服务器资源，以提高其服务质量并避免违反服务质量要求，或是减少分配给延迟敏感型任务的某类资源以提高批处理任务性能并提高资源利用率。环境产生的奖励信号有多种设计目标。若此动作的执行导致资源敏感型任务违反其服务质量要求，智能体将给予惩罚（负的奖励）；在不违反服务质量要求时，延迟敏感型任务占用的资源越少，智能体收到的奖励越大；所有的动作并不都一定可以在所以环境状态下执行，例如某类资源分配无法增加或减少，智能体若做出非法动作将会收到惩罚。

将强化学习应用在数据中心的资源调度上时，同样面临挑战。首先是功劳分配问题（Temporal Credit Assignment），智能体需要学习如何从直接观察到的奖励信号中分析以前所执行的动作哪些有正面的贡献，哪些产生了负面的影响，以及这些负面的贡献和影响各有多大。在一些情况中，一些能立即产生高额奖励的动作可能将使系统进入不理想的状态，难以产生更多的奖励；而另一些情况中，一个动作可能无法产生立即的奖励，但可能对系统进入一个理想状态至关重要。因此最优的策略需要产生长远的规划，智能体需要能够预见未来的可能情况才能采取相应的行动以最大化总的奖励。其次是探索环境的积极性。智能体需要探索环境，学习不同状态下执行不同动作所带来的后果。一方面，积极的探索环境让智能体可能学会更高效的控制策略；另一方面，智能体在任何时刻也应该尽量执行以找到的最优策略。对环境的探索太少将导致智能体始终执行没有足够优化的策略；而过多的探索将使智能体长时间执行不够优化的策略，拉长训练时间。最后是泛化问题，环境的状态空间随考虑的因素指数增长，表示环境状态的变量可能相当大。在这种情况下，智能体很可能根本无法在此遇到以前经历过的环境状态。因此，从智能体的经验中学习并泛化是唯一办法。

\section{强化学习方法简介}
\subsection{马可夫决策过程（Markov Decision Processes, MDPs）}
考虑一般的强化学习场景，如图\ref{fig:rl}（a）所示，在每一步骤$\mathrm{t}$能观察到环境状态$\mathrm{s_t}$。当智能体执行动作$\mathrm{a_t}$后，环境状态变为$\mathrm{s_{t+1}}$，且智能体收到奖励$\mathrm{r_t}$。环境状态的转变和智能体收到的奖励都是随机的，并被假设具有马尔可夫性质（Markov Property）。因此，智能体所处的环境被抽象为马可夫决策过程（Markov Decision Processes，MDPs）。一个马可夫决策过程包含了一个状态集合$\mathrm{S}$，一个动作集合$\mathrm{A}$，一个状态间转换的关于动作的条件概率分布$\mathrm{T=P(s_{t+1}=s'|s_t=s, a_t = a)}$，以及在每个状态下执行一个动作使得环境状态改变至另一个状态的奖励的期望$\mathrm{R=E[r_t+1|s_t=s, a_t=a, s_t+1=s']}$。

\begin{figure}
  \centering
  \begin{minipage}[b]{0.7\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{rl/mdp.png}
    \captionof{figure}{马尔可夫决策过程示例}
    \label{fig:mdp}
  \end{minipage}     
\end{figure}

图\ref{fig:mdp}是一个马尔可夫决策过程的示例。示例的马尔可夫决策过程定义了两个环境状态$S0$和$S1$，和智能体在两个状态下都可选择执行的两个动作$A0$和$A1$。智能体在状态$S0$下执行动作$A0$，有$0.1$的可能性留在状态$S0$并获得$3$的奖励；同时有$0.9$的可能性转移至状态$S1$并获得$15$的奖励。智能体能控制的仅仅是选择下一个执行的动作，并且对环境可能的反应没有先验的只是。因此智能体的目标在于优化自身选择动作的策略以获得最大的累积奖赏。值得注意的是，对于智能体来说，执行某一动作后的状态转变并不总是确定的，有时是按概率跳转的。环境状态如何变化不在智能体的控制范围内，但智能体能通过大量的经验学习到其变化的规律。

用强化学习框架解决资源调度问题时，很自然会将总的步骤设为无限步骤，在无限长的时间里智能体通过合理分配服务器资源给延迟敏感型任务以保证其服务质量要求同时尽量提高服务器资源利用率。随着智能体考虑的时间由有限变为无限，一个潜在的问题是劣质的策略也能带来无限的总效用，导致智能体无法定义一个有效的优化目标。因此，在无限时间步骤时，优化折现（discounted）后的累积奖赏更为合适。

  \begin{equation}
    \label{eq:discounted_reward}
    R = E(\sum_{n=1}^{\infty} {\gamma^{t}} \cdot r_{t+i})
  \end{equation}

\subsection{策略与策略梯度方法（Policy Gradient Methods）}

% and why not

\subsection{Q学习}
bellman

\subsection{DQN -> Actor Critic：}
graph